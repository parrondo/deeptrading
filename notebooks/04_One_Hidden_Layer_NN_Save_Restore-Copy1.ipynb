{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing an one hidden layer Neural Network with save and restore\n",
    "\n",
    "\n",
    "![one_layer_network](../images/one_layer_network.png)\n",
    "\n",
    "\n",
    "The progress of the model can be saved during and after training. This means that a model can be resumed where it left off and avoid long training times. Saving also means that you can share your model and others can recreate your work.\n",
    "\n",
    "We will illustrate how to create an one hidden layer NN, save it and make predictions with trained model after reload it.\n",
    "\n",
    "We will use the iris data for this exercise.\n",
    "\n",
    "We will build a one-hidden layer neural network  to predict the fourth attribute, Petal Width from the other three (Sepal length, Sepal width, Petal length).\n",
    "\n",
    "There are several differences with respect to the example before in order to ilustrate more Tensorflow possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from tensorflow.python.framework import ops\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of dataset\n",
      "n= 150 p= 3\n"
     ]
    }
   ],
   "source": [
    "# Before getting into pandas dataframes we will load an example dataset from sklearn library \n",
    "# type(data) #iris is a bunch instance which is inherited from dictionary\n",
    "data = load_iris() #load iris dataset\n",
    "\n",
    "# We get a pandas dataframe to better visualize the datasets\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "X_raw = np.array([x[0:3] for x in data.data])\n",
    "y_raw = np.array([x[3] for x in data.data])\n",
    "\n",
    "# Dimensions of dataset\n",
    "print(\"Dimensions of dataset\")\n",
    "n = X_raw.shape[0]\n",
    "p = X_raw.shape[1]\n",
    "print(\"n=\",n,\"p=\",p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['target', 'target_names', 'feature_names', 'data', 'DESCR'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys() #keys of the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_raw.shape # Array 150x3. Each element is a 3-dimensional data point: sepal length, sepal width, petal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_raw.shape # Vector 150. Each element is a 1-dimensional (scalar) data point: petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.1               3.5                1.4               0.2\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "4                  5.0               3.6                1.4               0.2\n",
       "5                  5.4               3.9                1.7               0.4\n",
       "6                  4.6               3.4                1.4               0.3\n",
       "7                  5.0               3.4                1.5               0.2\n",
       "8                  4.4               2.9                1.4               0.2\n",
       "9                  4.9               3.1                1.5               0.1\n",
       "10                 5.4               3.7                1.5               0.2\n",
       "11                 4.8               3.4                1.6               0.2\n",
       "12                 4.8               3.0                1.4               0.1\n",
       "13                 4.3               3.0                1.1               0.1\n",
       "14                 5.8               4.0                1.2               0.2\n",
       "15                 5.7               4.4                1.5               0.4\n",
       "16                 5.4               3.9                1.3               0.4\n",
       "17                 5.1               3.5                1.4               0.3\n",
       "18                 5.7               3.8                1.7               0.3\n",
       "19                 5.1               3.8                1.5               0.3\n",
       "20                 5.4               3.4                1.7               0.2\n",
       "21                 5.1               3.7                1.5               0.4\n",
       "22                 4.6               3.6                1.0               0.2\n",
       "23                 5.1               3.3                1.7               0.5\n",
       "24                 4.8               3.4                1.9               0.2\n",
       "25                 5.0               3.0                1.6               0.2\n",
       "26                 5.0               3.4                1.6               0.4\n",
       "27                 5.2               3.5                1.5               0.2\n",
       "28                 5.2               3.4                1.4               0.2\n",
       "29                 4.7               3.2                1.6               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "120                6.9               3.2                5.7               2.3\n",
       "121                5.6               2.8                4.9               2.0\n",
       "122                7.7               2.8                6.7               2.0\n",
       "123                6.3               2.7                4.9               1.8\n",
       "124                6.7               3.3                5.7               2.1\n",
       "125                7.2               3.2                6.0               1.8\n",
       "126                6.2               2.8                4.8               1.8\n",
       "127                6.1               3.0                4.9               1.8\n",
       "128                6.4               2.8                5.6               2.1\n",
       "129                7.2               3.0                5.8               1.6\n",
       "130                7.4               2.8                6.1               1.9\n",
       "131                7.9               3.8                6.4               2.0\n",
       "132                6.4               2.8                5.6               2.2\n",
       "133                6.3               2.8                5.1               1.5\n",
       "134                6.1               2.6                5.6               1.4\n",
       "135                7.7               3.0                6.1               2.3\n",
       "136                6.3               3.4                5.6               2.4\n",
       "137                6.4               3.1                5.5               1.8\n",
       "138                6.0               3.0                4.8               1.8\n",
       "139                6.9               3.1                5.4               2.1\n",
       "140                6.7               3.1                5.6               2.4\n",
       "141                6.9               3.1                5.1               2.3\n",
       "142                5.8               2.7                5.1               1.9\n",
       "143                6.8               3.2                5.9               2.3\n",
       "144                6.7               3.3                5.7               2.5\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Leave in blanck intentionally\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples:  150 \n",
      "Samples in train set:  105 \n",
      "Samples in test set:  45\n",
      "X_train.shape =  (105, 3) y_train.shape = (105,) \n",
      "X_test.shape =   (45, 3) y_test.shape =  (45,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "\n",
    "# Total samples\n",
    "nsamples = n\n",
    "\n",
    "# Splitting into train (70%) and test (30%) sets\n",
    "split = 70 # training split% ; test (100-split)%\n",
    "jindex = nsamples*split//100 # Index for slicing the samples\n",
    "\n",
    "# Samples in train\n",
    "nsamples_train = jindex\n",
    "\n",
    "# Samples in test\n",
    "nsamples_test = nsamples - nsamples_train\n",
    "print(\"Total number of samples: \",nsamples,\"\\nSamples in train set: \", nsamples_train,\n",
    "      \"\\nSamples in test set: \",nsamples_test)\n",
    "\n",
    "# Here are train and test samples\n",
    "X_train = X_raw[:jindex, :]\n",
    "y_train = y_raw[:jindex]\n",
    "\n",
    "X_test = X_raw[jindex:, :]\n",
    "y_test = y_raw[jindex:]\n",
    "\n",
    "print(\"X_train.shape = \", X_train.shape, \"y_train.shape =\", y_train.shape, \"\\nX_test.shape =  \",\n",
    "      X_test.shape, \"y_test.shape = \", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "Becareful not to write `X_test_std = sc.fit_transform(X_test)` instead of `X_test_std = sc.transform(X_test)`. In this case, it wouldn't make a great difference since the mean and standard deviation of the test set should be (quite) similar to the training set. However, this is not always the case in Forex market data, as has been well stablished in literature. The correct way is to re-use parameters from the training set if we are doing any kind of transformation. So, the test set should basically stand for \"new, unseen\" data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "y_train_std = sc.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_std = sc.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clears the default graph stack and resets the global default graph\n",
    "ops.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholders\n",
      "Initializers\n"
     ]
    }
   ],
   "source": [
    "# make results reproducible\n",
    "seed = 2\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)  \n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.005\n",
    "batch_size = 50\n",
    "n_features = X_train.shape[1]#  Number of features in training data\n",
    "epochs = 1000\n",
    "display_step = 50\n",
    "model_path = \"/tmp/model.ckpt\"\n",
    "n_classes = 1\n",
    "\n",
    "# Network Parameters\n",
    "# See figure of the model\n",
    "d0 = D = n_features # Layer 0 (Input layer number of features)\n",
    "d1 = 10 # Layer 1 (1st hidden layer number of features. Selected 10 for this example)\n",
    "d2 = C = 1 # Layer 2 (Output layer)\n",
    "\n",
    "# tf Graph input\n",
    "print(\"Placeholders\")\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, n_features], name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None,n_classes], name=\"y\")\n",
    "\n",
    "\n",
    "# Initializers\n",
    "print(\"Initializers\")\n",
    "sigma = 1\n",
    "weight_initializer = tf.variance_scaling_initializer(mode=\"fan_avg\", distribution=\"uniform\", scale=sigma)\n",
    "bias_initializer = tf.zeros_initializer()\n",
    "\n",
    "# Create model\n",
    "def onelayer_perceptron(X, variables):\n",
    "    # Hidden layer with ReLU activation\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(X, variables['W1']), variables['bias1']))\n",
    "    # Output layer with ReLU activation\n",
    "    out_layer = tf.nn.relu(tf.add(tf.matmul(layer_1, variables['W2']), variables['bias2']))\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "variables = {\n",
    "    'W1': tf.Variable(weight_initializer([n_features, d1]), name=\"W1\"), # inputs -> hidden neurons\n",
    "    'bias1': tf.Variable(bias_initializer([d1]), name=\"bias1\"), # one biases for each hidden neurons\n",
    "    'W2': tf.Variable(weight_initializer([d1, d2]), name=\"W2\"), # hidden inputs -> 1 output\n",
    "    'bias2': tf.Variable(bias_initializer([d2]), name=\"bias2\") # 1 bias for the output\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "y_hat = onelayer_perceptron(X, variables)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.square(y - y_hat)) # MSE\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss) # Train step\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model  and Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1st session...\n",
      "Epoch: 0050 Loss= 0.830693305\n",
      "Epoch: 0100 Loss= 0.672560811\n",
      "Epoch: 0150 Loss= 0.643242896\n",
      "Epoch: 0200 Loss= 0.534627974\n",
      "Epoch: 0250 Loss= 0.641512394\n",
      "Epoch: 0300 Loss= 0.469293058\n",
      "Epoch: 0350 Loss= 0.633833647\n",
      "Epoch: 0400 Loss= 0.606160522\n",
      "Epoch: 0450 Loss= 0.658931136\n",
      "Epoch: 0500 Loss= 0.604701042\n",
      "Epoch: 0550 Loss= 0.559300005\n",
      "Epoch: 0600 Loss= 0.577932715\n",
      "Epoch: 0650 Loss= 0.593395472\n",
      "Epoch: 0700 Loss= 0.597412467\n",
      "Epoch: 0750 Loss= 0.545120120\n",
      "Epoch: 0800 Loss= 0.362084955\n",
      "Epoch: 0850 Loss= 0.492658347\n",
      "Epoch: 0900 Loss= 0.541678667\n",
      "Epoch: 0950 Loss= 0.527150929\n",
      "Epoch: 1000 Loss= 0.501759350\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "First Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Running first session\n",
    "print(\"Starting 1st session...\")\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Writer to record image, scalar, histogram and graph for display in tensorboard\n",
    "    writer = tf.summary.FileWriter(\"/tmp/tensorflow_logs\", sess.graph)  # create writer\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        rand_index = np.random.choice(len(X_train), size=batch_size)\n",
    "        X_rand = X_train_std[rand_index]\n",
    "        y_rand = y_train_std[rand_index]\n",
    "        #y_rand = np.transpose([y_train[rand_index]])\n",
    "        sess.run(optimizer, feed_dict={X: X_rand, y: y_rand})\n",
    "\n",
    "        train_temp_loss = sess.run(loss, feed_dict={X: X_rand, y: y_rand})\n",
    "        train_loss.append(np.sqrt(train_temp_loss))\n",
    "    \n",
    "        test_temp_loss = sess.run(loss, feed_dict={X: X_test_std, y: y_test_std})\n",
    "        test_loss.append(np.sqrt(test_temp_loss))\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"Loss=\", \\\n",
    "                \"{:.9f}\".format(train_temp_loss))\n",
    "\n",
    "    # Close writer\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "        \n",
    "    # Save model weights to disk\n",
    "    save_path = saver.save(sess, model_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    print(\"First Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Plot loss (MSE) over time\n",
    "plt.plot(train_loss, 'k-', label='Train Loss')\n",
    "plt.plot(test_loss, 'r--', label='Test Loss')\n",
    "plt.title('Loss (MSE) per Generation')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard Graph\n",
    "\n",
    "\n",
    "What follows is the graph we have executed and all data about it. Note the \"save\" label.\n",
    "\n",
    "\n",
    "![graph_3](../images/graph_3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a Tensorflow model\n",
    "\n",
    "So, now we have our model saved.\n",
    "\n",
    "Tensorflow model has four main files:\n",
    "* a) Meta graph:\n",
    "This is a protocol buffer which saves the complete Tensorflow graph; i.e. all variables, operations, collections etc. This file has .meta extension.\n",
    "\n",
    "\n",
    "* b) y c) Checkpoint files:\n",
    "It is a binary file which contains all the values of the weights, biases, gradients and all the other variables saved. Tensorflow has changed from version 0.11. Instead of a single .ckpt file, we have now two files: .index and .data file that contains our training variables. \n",
    "\n",
    "\n",
    "* d) Along with this, Tensorflow also has a file named checkpoint which simply keeps a record of latest checkpoint files saved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the model\n",
    "\n",
    "\n",
    "We can retrain the model as many times as we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 2nd session...\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model restored from file: /tmp/model.ckpt\n",
      "Epoch: 0050 Loss= 0.501028776\n",
      "Epoch: 0100 Loss= 0.526981473\n",
      "Epoch: 0150 Loss= 0.495494872\n",
      "Epoch: 0200 Loss= 0.403977782\n",
      "Epoch: 0250 Loss= 0.625274420\n",
      "Epoch: 0300 Loss= 0.595013261\n",
      "Epoch: 0350 Loss= 0.390508205\n",
      "Epoch: 0400 Loss= 0.455603242\n",
      "Epoch: 0450 Loss= 0.538071275\n",
      "Epoch: 0500 Loss= 0.259512991\n",
      "Epoch: 0550 Loss= 0.403911054\n",
      "Epoch: 0600 Loss= 0.494841188\n",
      "Epoch: 0650 Loss= 0.488838494\n",
      "Epoch: 0700 Loss= 0.527527213\n",
      "Epoch: 0750 Loss= 0.404275924\n",
      "Epoch: 0800 Loss= 0.450985551\n",
      "Epoch: 0850 Loss= 0.325873464\n",
      "Epoch: 0900 Loss= 0.520956397\n",
      "Epoch: 0950 Loss= 0.406218350\n",
      "Epoch: 1000 Loss= 0.380938828\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Second Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Running a new session\n",
    "print(\"Starting 2nd session...\")\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "\n",
    "    # Restore model weights from previously saved model\n",
    "    saver.restore(sess, model_path)\n",
    "    print(\"Model restored from file: %s\" % model_path)\n",
    "\n",
    "    # Resume training\n",
    "    for epoch in range(epochs):\n",
    "        rand_index = np.random.choice(len(X_train), size=batch_size)\n",
    "        X_rand = X_train_std[rand_index]\n",
    "        y_rand = y_train_std[rand_index]\n",
    "        #y_rand = np.transpose([y_train[rand_index]])\n",
    "        sess.run(optimizer, feed_dict={X: X_rand, y: y_rand})\n",
    "\n",
    "        train_temp_loss = sess.run(loss, feed_dict={X: X_rand, y: y_rand})\n",
    "        train_loss.append(np.sqrt(train_temp_loss))\n",
    "    \n",
    "        test_temp_loss = sess.run(loss, feed_dict={X: X_test_std, y: y_test_std})\n",
    "        test_loss.append(np.sqrt(test_temp_loss))\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"Loss=\", \\\n",
    "                \"{:.9f}\".format(train_temp_loss))\n",
    "\n",
    "    # Close writer\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "    # Save model weights to disk\n",
    "    save_path = saver.save(sess, model_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    print(\"Second Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the model to make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction session...\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model restored from file: /tmp/model.ckpt\n",
      "[[1.2619315]\n",
      " [1.2590624]\n",
      " [4.0073366]]\n"
     ]
    }
   ],
   "source": [
    "# Running a new session for predictions\n",
    "print(\"Starting prediction session...\")\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "\n",
    "    # Restore model weights from previously saved model\n",
    "    saver.restore(sess, model_path)\n",
    "    print(\"Model restored from file: %s\" % model_path)\n",
    "\n",
    "    # We try to predict the petal width (cm) of three samples\n",
    "    #Caution!!! This data are not the right data (see below why)\n",
    "    feed_dict = {X: [[5.1, 3.5, 1.4],\n",
    "                     [4.8, 3.0, 1.4],\n",
    "                     [6.3, 3.4, 5.6]]\n",
    "                }\n",
    "    prediction = sess.run(y_hat, feed_dict)\n",
    "    print(prediction) # True value 0.2, 0.1, 2.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, not very good results. But it is worst that we could think!\n",
    "Data are not right because we have trained our model with a transformed data (standardization) and now we must\n",
    "use again transformed data to make predictions. Also wi will get back transformed data again. So, we must inverse the transformation to get the original kind of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First: transform our original data. The data we want to make the prediction about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = [[5.1, 3.5, 1.4],\n",
    "          [4.8, 3.0, 1.4],\n",
    "          [6.3, 3.4, 5.6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.86549436, 4.28228483, 0.89182234],\n",
       "       [6.38114257, 3.47503186, 0.89182234],\n",
       "       [8.8029015 , 4.12083424, 7.67274733]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pred_std = sc.transform(X_pred)\n",
    "X_pred_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second: we are ready to make the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction session...\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model restored from file: /tmp/model.ckpt\n",
      "[[1.0284168]\n",
      " [1.0237845]\n",
      " [5.3811445]]\n"
     ]
    }
   ],
   "source": [
    "# Running a new session for predictions\n",
    "print(\"Starting prediction session...\")\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "\n",
    "    # Restore model weights from previously saved model\n",
    "    saver.restore(sess, model_path)\n",
    "    print(\"Model restored from file: %s\" % model_path)\n",
    "\n",
    "    # We try to predict the petal width (cm) of three samples\n",
    "    feed_dict_std = {X: [[6.86549436, 4.28228483, 0.89182234],\n",
    "       [6.38114257, 3.47503186, 0.89182234],\n",
    "       [8.8029015 , 4.12083424, 7.67274733]]}\n",
    "    prediction = sess.run(y_hat, feed_dict_std)\n",
    "    print(prediction) # True value 0.2, 0.1, 2.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third: we reverse the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4846045],\n",
       "       [1.4817353],\n",
       "       [4.1806164]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_rev = sc.inverse_transform(prediction)\n",
    "y_hat_rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. We'll try to improve them with a deeper network. That is the goal of the next notebook."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deeptrading",
   "language": "python",
   "name": "deeptrading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
