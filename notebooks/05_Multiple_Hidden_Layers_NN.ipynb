{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a multiple hidden layer Neural Network \n",
    "\n",
    "\n",
    "![deep_network_model](../images/deep_network_model_avatar.jpg)\n",
    "\n",
    "\n",
    "The progress of the model can be saved during and after training. This means that a model can be resumed where it left off and avoid long training times. Saving also means that you can share your model and others can recreate your work.\n",
    "\n",
    "We will illustrate how to create a multiple fully connected hidden layer NN, save it and make predictions with trained model after reload it.\n",
    "\n",
    "We will use the iris data for this exercise.\n",
    "\n",
    "We will build a three-hidden layer neural network  to predict the fourth attribute, Petal Width from the other three (Sepal length, Sepal width, Petal length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/parrondo/anaconda3/envs/deeptrading/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from tensorflow.python.framework import ops\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of dataset\n",
      "n= 150 p= 3\n"
     ]
    }
   ],
   "source": [
    "# Before getting into pandas dataframes we will load an example dataset from sklearn library \n",
    "# type(data) #iris is a bunch instance which is inherited from dictionary\n",
    "data = load_iris() #load iris dataset\n",
    "\n",
    "# We get a pandas dataframe to better visualize the datasets\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "X_raw = np.array([x[0:3] for x in data.data])\n",
    "y_raw = np.array([x[3] for x in data.data])\n",
    "\n",
    "# Dimensions of dataset\n",
    "print(\"Dimensions of dataset\")\n",
    "n = X_raw.shape[0]\n",
    "p = X_raw.shape[1]\n",
    "print(\"n=\",n,\"p=\",p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['target', 'DESCR', 'feature_names', 'target_names', 'data'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys() #keys of the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_raw.shape # Array 150x3. Each element is a 3-dimensional data point: sepal length, sepal width, petal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_raw.shape # Vector 150. Each element is a 1-dimensional (scalar) data point: petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.1               3.5                1.4               0.2\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "4                  5.0               3.6                1.4               0.2\n",
       "5                  5.4               3.9                1.7               0.4\n",
       "6                  4.6               3.4                1.4               0.3\n",
       "7                  5.0               3.4                1.5               0.2\n",
       "8                  4.4               2.9                1.4               0.2\n",
       "9                  4.9               3.1                1.5               0.1\n",
       "10                 5.4               3.7                1.5               0.2\n",
       "11                 4.8               3.4                1.6               0.2\n",
       "12                 4.8               3.0                1.4               0.1\n",
       "13                 4.3               3.0                1.1               0.1\n",
       "14                 5.8               4.0                1.2               0.2\n",
       "15                 5.7               4.4                1.5               0.4\n",
       "16                 5.4               3.9                1.3               0.4\n",
       "17                 5.1               3.5                1.4               0.3\n",
       "18                 5.7               3.8                1.7               0.3\n",
       "19                 5.1               3.8                1.5               0.3\n",
       "20                 5.4               3.4                1.7               0.2\n",
       "21                 5.1               3.7                1.5               0.4\n",
       "22                 4.6               3.6                1.0               0.2\n",
       "23                 5.1               3.3                1.7               0.5\n",
       "24                 4.8               3.4                1.9               0.2\n",
       "25                 5.0               3.0                1.6               0.2\n",
       "26                 5.0               3.4                1.6               0.4\n",
       "27                 5.2               3.5                1.5               0.2\n",
       "28                 5.2               3.4                1.4               0.2\n",
       "29                 4.7               3.2                1.6               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "120                6.9               3.2                5.7               2.3\n",
       "121                5.6               2.8                4.9               2.0\n",
       "122                7.7               2.8                6.7               2.0\n",
       "123                6.3               2.7                4.9               1.8\n",
       "124                6.7               3.3                5.7               2.1\n",
       "125                7.2               3.2                6.0               1.8\n",
       "126                6.2               2.8                4.8               1.8\n",
       "127                6.1               3.0                4.9               1.8\n",
       "128                6.4               2.8                5.6               2.1\n",
       "129                7.2               3.0                5.8               1.6\n",
       "130                7.4               2.8                6.1               1.9\n",
       "131                7.9               3.8                6.4               2.0\n",
       "132                6.4               2.8                5.6               2.2\n",
       "133                6.3               2.8                5.1               1.5\n",
       "134                6.1               2.6                5.6               1.4\n",
       "135                7.7               3.0                6.1               2.3\n",
       "136                6.3               3.4                5.6               2.4\n",
       "137                6.4               3.1                5.5               1.8\n",
       "138                6.0               3.0                4.8               1.8\n",
       "139                6.9               3.1                5.4               2.1\n",
       "140                6.7               3.1                5.6               2.4\n",
       "141                6.9               3.1                5.1               2.3\n",
       "142                5.8               2.7                5.1               1.9\n",
       "143                6.8               3.2                5.9               2.3\n",
       "144                6.7               3.3                5.7               2.5\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Leave in blanck intentionally\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples:  150 \n",
      "Samples in train set:  105 \n",
      "Samples in test set:  45\n",
      "X_train.shape =  (105, 3) y_train.shape = (105,) \n",
      "X_test.shape =   (45, 3) y_test.shape =  (45,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "\n",
    "# Total samples\n",
    "nsamples = n\n",
    "\n",
    "# Splitting into train (70%) and test (30%) sets\n",
    "split = 70 # training split% ; test (100-split)%\n",
    "jindex = nsamples*split//100 # Index for slicing the samples\n",
    "\n",
    "# Samples in train\n",
    "nsamples_train = jindex\n",
    "\n",
    "# Samples in test\n",
    "nsamples_test = nsamples - nsamples_train\n",
    "print(\"Total number of samples: \",nsamples,\"\\nSamples in train set: \", nsamples_train,\n",
    "      \"\\nSamples in test set: \",nsamples_test)\n",
    "\n",
    "# Here are train and test samples\n",
    "X_train = X_raw[:jindex, :]\n",
    "y_train = y_raw[:jindex]\n",
    "\n",
    "X_test = X_raw[jindex:, :]\n",
    "y_test = y_raw[jindex:]\n",
    "\n",
    "print(\"X_train.shape = \", X_train.shape, \"y_train.shape =\", y_train.shape, \"\\nX_test.shape =  \",\n",
    "      X_test.shape, \"y_test.shape = \", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "Becareful not to write `X_test_std = sc.fit_transform(X_test)` instead of `X_test_std = sc.transform(X_test)`. In this case, it wouldn't make a great difference since the mean and standard deviation of the test set should be (quite) similar to the training set. However, this is not always the case in Forex market data, as has been well stablished in literature. The correct way is to re-use parameters from the training set if we are doing any kind of transformation. So, the test set should basically stand for \"new, unseen\" data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "y_train_std = sc.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_std = sc.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.1, 1.7, 1.8, 1.8, 2.5, 2. , 1.9, 2.1, 2. , 2.4, 2.3, 1.8, 2.2,\n",
       "       2.3, 1.5, 2.3, 2. , 2. , 1.8, 2.1, 1.8, 1.8, 1.8, 2.1, 1.6, 1.9,\n",
       "       2. , 2.2, 1.5, 1.4, 2.3, 2.4, 1.8, 1.8, 2.1, 2.4, 2.3, 1.9, 2.3,\n",
       "       2.5, 2.3, 1.9, 2. , 2.3, 1.8])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.1],\n",
       "       [1.7],\n",
       "       [1.8],\n",
       "       [1.8],\n",
       "       [2.5],\n",
       "       [2. ],\n",
       "       [1.9],\n",
       "       [2.1],\n",
       "       [2. ],\n",
       "       [2.4],\n",
       "       [2.3],\n",
       "       [1.8],\n",
       "       [2.2],\n",
       "       [2.3],\n",
       "       [1.5],\n",
       "       [2.3],\n",
       "       [2. ],\n",
       "       [2. ],\n",
       "       [1.8],\n",
       "       [2.1],\n",
       "       [1.8],\n",
       "       [1.8],\n",
       "       [1.8],\n",
       "       [2.1],\n",
       "       [1.6],\n",
       "       [1.9],\n",
       "       [2. ],\n",
       "       [2.2],\n",
       "       [1.5],\n",
       "       [1.4],\n",
       "       [2.3],\n",
       "       [2.4],\n",
       "       [1.8],\n",
       "       [1.8],\n",
       "       [2.1],\n",
       "       [2.4],\n",
       "       [2.3],\n",
       "       [1.9],\n",
       "       [2.3],\n",
       "       [2.5],\n",
       "       [2.3],\n",
       "       [1.9],\n",
       "       [2. ],\n",
       "       [2.3],\n",
       "       [1.8]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.inverse_transform(y_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clears the default graph stack and resets the global default graph\n",
    "ops.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholders\n",
      "Initializers\n"
     ]
    }
   ],
   "source": [
    "# make results reproducible\n",
    "seed = 2\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)  \n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.005\n",
    "batch_size = 50\n",
    "n_features = X_train.shape[1]#  Number of features in training data\n",
    "epochs = 1000\n",
    "display_step = 50\n",
    "model_path = \"/tmp/model.ckpt\"\n",
    "n_classes = 1\n",
    "\n",
    "# Network Parameters\n",
    "# See figure of the model\n",
    "d0 = D = n_features # Layer 0 (Input layer number of features)\n",
    "d1 = 100 # Layer 1 (50 hidden nodes)\n",
    "d2 = 50 # Layer 2 (25 hidden nodes) \n",
    "d3 = 5 # Layer 3 (5 hidden nodes)\n",
    "d4 = C = 1 # Layer 4 (Output layer)\n",
    "\n",
    "# tf Graph input\n",
    "print(\"Placeholders\")\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, n_features], name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None,n_classes], name=\"y\")\n",
    "\n",
    "\n",
    "# Initializers\n",
    "print(\"Initializers\")\n",
    "sigma = 1\n",
    "weight_initializer = tf.variance_scaling_initializer(mode=\"fan_avg\", distribution=\"uniform\", scale=sigma)\n",
    "bias_initializer = tf.zeros_initializer()\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(X, variables):\n",
    "    # Hidden layer with ReLU activation\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(X, variables['W1']), variables['bias1']))\n",
    "    # Hidden layer with ReLU activation\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, variables['W2']), variables['bias2']))\n",
    "    # Hidden layer with ReLU activation\n",
    "    layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, variables['W3']), variables['bias3']))\n",
    "    # Output layer with ReLU activation\n",
    "    out_layer = tf.nn.relu(tf.add(tf.matmul(layer_3, variables['W4']), variables['bias4']))\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "variables = {\n",
    "    'W1': tf.Variable(weight_initializer([n_features, d1]), name=\"W1\"), # inputs -> d1 hidden neurons\n",
    "    'bias1': tf.Variable(bias_initializer([d1]), name=\"bias1\"), # one biases for each d1 hidden neurons\n",
    "    'W2': tf.Variable(weight_initializer([d1, d2]), name=\"W2\"), # d1 hidden inputs -> d2 hidden neurons\n",
    "    'bias2': tf.Variable(bias_initializer([d2]), name=\"bias2\"), # one biases for each d2 hidden neurons\n",
    "    'W3': tf.Variable(weight_initializer([d2, d3]), name=\"W3\"), ## d2 hidden inputs -> d3 hidden neurons\n",
    "    'bias3': tf.Variable(bias_initializer([d3]), name=\"bias3\"), # one biases for each d3 hidden neurons\n",
    "    'W4': tf.Variable(weight_initializer([d3, d4]), name=\"W4\"), # d3 hidden inputs -> 1 output\n",
    "    'bias4': tf.Variable(bias_initializer([d4]), name=\"bias4\") # 1 bias for the output\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "y_hat = multilayer_perceptron(X, variables)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.square(y - y_hat)) # MSE\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss) # Train step\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model  and Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1st session...\n",
      "Epoch: 0050 Lost= 0.121212967\n",
      "Epoch: 0100 Lost= 0.087612927\n",
      "Epoch: 0150 Lost= 0.054805990\n",
      "Epoch: 0200 Lost= 0.042091131\n",
      "Epoch: 0250 Lost= 0.033010442\n",
      "Epoch: 0300 Lost= 0.028964423\n",
      "Epoch: 0350 Lost= 0.033457872\n",
      "Epoch: 0400 Lost= 0.024154516\n",
      "Epoch: 0450 Lost= 0.033675905\n",
      "Epoch: 0500 Lost= 0.020445058\n",
      "Epoch: 0550 Lost= 0.022125030\n",
      "Epoch: 0600 Lost= 0.015964977\n",
      "Epoch: 0650 Lost= 0.014663436\n",
      "Epoch: 0700 Lost= 0.011865783\n",
      "Epoch: 0750 Lost= 0.014667089\n",
      "Epoch: 0800 Lost= 0.018193373\n",
      "Epoch: 0850 Lost= 0.014791524\n",
      "Epoch: 0900 Lost= 0.012803431\n",
      "Epoch: 0950 Lost= 0.011303975\n",
      "Epoch: 1000 Lost= 0.011635998\n",
      "Epoch: 1050 Lost= 0.016628481\n",
      "Epoch: 1100 Lost= 0.011171184\n",
      "Epoch: 1150 Lost= 0.014480609\n",
      "Epoch: 1200 Lost= 0.010597562\n",
      "Epoch: 1250 Lost= 0.009532723\n",
      "Epoch: 1300 Lost= 0.010108640\n",
      "Epoch: 1350 Lost= 0.009826462\n",
      "Epoch: 1400 Lost= 0.013368943\n",
      "Epoch: 1450 Lost= 0.010665327\n",
      "Epoch: 1500 Lost= 0.008773899\n",
      "Epoch: 1550 Lost= 0.008465427\n",
      "Epoch: 1600 Lost= 0.013776772\n",
      "Epoch: 1650 Lost= 0.011079680\n",
      "Epoch: 1700 Lost= 0.011942017\n",
      "Epoch: 1750 Lost= 0.013662893\n",
      "Epoch: 1800 Lost= 0.010473980\n",
      "Epoch: 1850 Lost= 0.013280788\n",
      "Epoch: 1900 Lost= 0.013363536\n",
      "Epoch: 1950 Lost= 0.012372569\n",
      "Epoch: 2000 Lost= 0.014634406\n",
      "Epoch: 2050 Lost= 0.007925984\n",
      "Epoch: 2100 Lost= 0.007317244\n",
      "Epoch: 2150 Lost= 0.009368614\n",
      "Epoch: 2200 Lost= 0.009015526\n",
      "Epoch: 2250 Lost= 0.008901188\n",
      "Epoch: 2300 Lost= 0.011704995\n",
      "Epoch: 2350 Lost= 0.006703234\n",
      "Epoch: 2400 Lost= 0.009889710\n",
      "Epoch: 2450 Lost= 0.010166203\n",
      "Epoch: 2500 Lost= 0.007509411\n",
      "Epoch: 2550 Lost= 0.017735250\n",
      "Epoch: 2600 Lost= 0.009342163\n",
      "Epoch: 2650 Lost= 0.011587437\n",
      "Epoch: 2700 Lost= 0.009117064\n",
      "Epoch: 2750 Lost= 0.006910873\n",
      "Epoch: 2800 Lost= 0.008642302\n",
      "Epoch: 2850 Lost= 0.007138947\n",
      "Epoch: 2900 Lost= 0.008594126\n",
      "Epoch: 2950 Lost= 0.007688348\n",
      "Epoch: 3000 Lost= 0.011702564\n",
      "Epoch: 3050 Lost= 0.010366002\n",
      "Epoch: 3100 Lost= 0.009899775\n",
      "Epoch: 3150 Lost= 0.012533127\n",
      "Epoch: 3200 Lost= 0.008397793\n",
      "Epoch: 3250 Lost= 0.010190758\n",
      "Epoch: 3300 Lost= 0.007057909\n",
      "Epoch: 3350 Lost= 0.012476509\n",
      "Epoch: 3400 Lost= 0.007060330\n",
      "Epoch: 3450 Lost= 0.008135856\n",
      "Epoch: 3500 Lost= 0.007966333\n",
      "Epoch: 3550 Lost= 0.009871390\n",
      "Epoch: 3600 Lost= 0.011853478\n",
      "Epoch: 3650 Lost= 0.007584572\n",
      "Epoch: 3700 Lost= 0.009648617\n",
      "Epoch: 3750 Lost= 0.010483559\n",
      "Epoch: 3800 Lost= 0.013980133\n",
      "Epoch: 3850 Lost= 0.008503906\n",
      "Epoch: 3900 Lost= 0.009369498\n",
      "Epoch: 3950 Lost= 0.011840296\n",
      "Epoch: 4000 Lost= 0.007476908\n",
      "Epoch: 4050 Lost= 0.008243524\n",
      "Epoch: 4100 Lost= 0.011404473\n",
      "Epoch: 4150 Lost= 0.012074584\n",
      "Epoch: 4200 Lost= 0.009088356\n",
      "Epoch: 4250 Lost= 0.012826816\n",
      "Epoch: 4300 Lost= 0.006762956\n",
      "Epoch: 4350 Lost= 0.009018660\n",
      "Epoch: 4400 Lost= 0.006678068\n",
      "Epoch: 4450 Lost= 0.006246779\n",
      "Epoch: 4500 Lost= 0.008019404\n",
      "Epoch: 4550 Lost= 0.010728627\n",
      "Epoch: 4600 Lost= 0.011549286\n",
      "Epoch: 4650 Lost= 0.007602415\n",
      "Epoch: 4700 Lost= 0.006915380\n",
      "Epoch: 4750 Lost= 0.009931020\n",
      "Epoch: 4800 Lost= 0.008014765\n",
      "Epoch: 4850 Lost= 0.011243353\n",
      "Epoch: 4900 Lost= 0.007704717\n",
      "Epoch: 4950 Lost= 0.010802790\n",
      "Epoch: 5000 Lost= 0.007454729\n",
      "Epoch: 5050 Lost= 0.014608316\n",
      "Epoch: 5100 Lost= 0.007801782\n",
      "Epoch: 5150 Lost= 0.008521575\n",
      "Epoch: 5200 Lost= 0.009326904\n",
      "Epoch: 5250 Lost= 0.005555832\n",
      "Epoch: 5300 Lost= 0.008542498\n",
      "Epoch: 5350 Lost= 0.011351025\n",
      "Epoch: 5400 Lost= 0.011266726\n",
      "Epoch: 5450 Lost= 0.008595885\n",
      "Epoch: 5500 Lost= 0.008332850\n",
      "Epoch: 5550 Lost= 0.010628356\n",
      "Epoch: 5600 Lost= 0.009201619\n",
      "Epoch: 5650 Lost= 0.013969340\n",
      "Epoch: 5700 Lost= 0.008151679\n",
      "Epoch: 5750 Lost= 0.008839128\n",
      "Epoch: 5800 Lost= 0.008891174\n",
      "Epoch: 5850 Lost= 0.013180419\n",
      "Epoch: 5900 Lost= 0.011190291\n",
      "Epoch: 5950 Lost= 0.009019467\n",
      "Epoch: 6000 Lost= 0.007021047\n",
      "Epoch: 6050 Lost= 0.009353978\n",
      "Epoch: 6100 Lost= 0.006219963\n",
      "Epoch: 6150 Lost= 0.007817273\n",
      "Epoch: 6200 Lost= 0.009534100\n",
      "Epoch: 6250 Lost= 0.011781101\n",
      "Epoch: 6300 Lost= 0.008351857\n",
      "Epoch: 6350 Lost= 0.008990434\n",
      "Epoch: 6400 Lost= 0.007619111\n",
      "Epoch: 6450 Lost= 0.006375447\n",
      "Epoch: 6500 Lost= 0.006609187\n",
      "Epoch: 6550 Lost= 0.008024238\n",
      "Epoch: 6600 Lost= 0.009900345\n",
      "Epoch: 6650 Lost= 0.008371438\n",
      "Epoch: 6700 Lost= 0.007056421\n",
      "Epoch: 6750 Lost= 0.007992045\n",
      "Epoch: 6800 Lost= 0.007984796\n",
      "Epoch: 6850 Lost= 0.011936692\n",
      "Epoch: 6900 Lost= 0.010063800\n",
      "Epoch: 6950 Lost= 0.005043251\n",
      "Epoch: 7000 Lost= 0.007846035\n",
      "Epoch: 7050 Lost= 0.013405660\n",
      "Epoch: 7100 Lost= 0.011605227\n",
      "Epoch: 7150 Lost= 0.009365008\n",
      "Epoch: 7200 Lost= 0.008629457\n",
      "Epoch: 7250 Lost= 0.010157854\n",
      "Epoch: 7300 Lost= 0.009094646\n",
      "Epoch: 7350 Lost= 0.006827044\n",
      "Epoch: 7400 Lost= 0.005453494\n",
      "Epoch: 7450 Lost= 0.007780420\n",
      "Epoch: 7500 Lost= 0.011007525\n",
      "Epoch: 7550 Lost= 0.007301999\n",
      "Epoch: 7600 Lost= 0.008138284\n",
      "Epoch: 7650 Lost= 0.011067736\n",
      "Epoch: 7700 Lost= 0.008095294\n",
      "Epoch: 7750 Lost= 0.009447844\n",
      "Epoch: 7800 Lost= 0.008278218\n",
      "Epoch: 7850 Lost= 0.008111618\n",
      "Epoch: 7900 Lost= 0.009282346\n",
      "Epoch: 7950 Lost= 0.009363904\n",
      "Epoch: 8000 Lost= 0.010153383\n",
      "Epoch: 8050 Lost= 0.009936367\n",
      "Epoch: 8100 Lost= 0.009709898\n",
      "Epoch: 8150 Lost= 0.006814663\n",
      "Epoch: 8200 Lost= 0.009973403\n",
      "Epoch: 8250 Lost= 0.008899162\n",
      "Epoch: 8300 Lost= 0.007580366\n",
      "Epoch: 8350 Lost= 0.011870252\n",
      "Epoch: 8400 Lost= 0.003625249\n",
      "Epoch: 8450 Lost= 0.011714692\n",
      "Epoch: 8500 Lost= 0.004748984\n",
      "Epoch: 8550 Lost= 0.011266818\n",
      "Epoch: 8600 Lost= 0.009444613\n",
      "Epoch: 8650 Lost= 0.007004244\n",
      "Epoch: 8700 Lost= 0.008347324\n",
      "Epoch: 8750 Lost= 0.009974777\n",
      "Epoch: 8800 Lost= 0.008346334\n",
      "Epoch: 8850 Lost= 0.010198839\n",
      "Epoch: 8900 Lost= 0.012109483\n",
      "Epoch: 8950 Lost= 0.007765678\n",
      "Epoch: 9000 Lost= 0.010655931\n",
      "Epoch: 9050 Lost= 0.011906773\n",
      "Epoch: 9100 Lost= 0.009706748\n",
      "Epoch: 9150 Lost= 0.010370594\n",
      "Epoch: 9200 Lost= 0.007045568\n",
      "Epoch: 9250 Lost= 0.006348282\n",
      "Epoch: 9300 Lost= 0.008027204\n",
      "Epoch: 9350 Lost= 0.009342426\n",
      "Epoch: 9400 Lost= 0.007446849\n",
      "Epoch: 9450 Lost= 0.008174302\n",
      "Epoch: 9500 Lost= 0.009489000\n",
      "Epoch: 9550 Lost= 0.005829980\n",
      "Epoch: 9600 Lost= 0.009291637\n",
      "Epoch: 9650 Lost= 0.009061862\n",
      "Epoch: 9700 Lost= 0.006420088\n",
      "Epoch: 9750 Lost= 0.007777751\n",
      "Epoch: 9800 Lost= 0.007094600\n",
      "Epoch: 9850 Lost= 0.008545948\n",
      "Epoch: 9900 Lost= 0.009482973\n",
      "Epoch: 9950 Lost= 0.010582455\n",
      "Epoch: 10000 Lost= 0.008953589\n",
      "Epoch: 10050 Lost= 0.011766634\n",
      "Epoch: 10100 Lost= 0.006207764\n",
      "Epoch: 10150 Lost= 0.009411525\n",
      "Epoch: 10200 Lost= 0.008989490\n",
      "Epoch: 10250 Lost= 0.011497977\n",
      "Epoch: 10300 Lost= 0.010013548\n",
      "Epoch: 10350 Lost= 0.007899803\n",
      "Epoch: 10400 Lost= 0.006843876\n",
      "Epoch: 10450 Lost= 0.008318203\n",
      "Epoch: 10500 Lost= 0.008477792\n",
      "Epoch: 10550 Lost= 0.009094267\n",
      "Epoch: 10600 Lost= 0.011732170\n",
      "Epoch: 10650 Lost= 0.006271280\n",
      "Epoch: 10700 Lost= 0.008313767\n",
      "Epoch: 10750 Lost= 0.006342800\n",
      "Epoch: 10800 Lost= 0.007579850\n",
      "Epoch: 10850 Lost= 0.009039856\n",
      "Epoch: 10900 Lost= 0.010074376\n",
      "Epoch: 10950 Lost= 0.006894245\n",
      "Epoch: 11000 Lost= 0.006121863\n",
      "Epoch: 11050 Lost= 0.008163215\n",
      "Epoch: 11100 Lost= 0.008375361\n",
      "Epoch: 11150 Lost= 0.009943899\n",
      "Epoch: 11200 Lost= 0.014519849\n",
      "Epoch: 11250 Lost= 0.007848904\n",
      "Epoch: 11300 Lost= 0.007455700\n",
      "Epoch: 11350 Lost= 0.004963035\n",
      "Epoch: 11400 Lost= 0.007248364\n",
      "Epoch: 11450 Lost= 0.012121891\n",
      "Epoch: 11500 Lost= 0.009286264\n",
      "Epoch: 11550 Lost= 0.011158308\n",
      "Epoch: 11600 Lost= 0.011643532\n",
      "Epoch: 11650 Lost= 0.010070193\n",
      "Epoch: 11700 Lost= 0.007157684\n",
      "Epoch: 11750 Lost= 0.009122388\n",
      "Epoch: 11800 Lost= 0.008513424\n",
      "Epoch: 11850 Lost= 0.006894275\n",
      "Epoch: 11900 Lost= 0.010077077\n",
      "Epoch: 11950 Lost= 0.011013249\n",
      "Epoch: 12000 Lost= 0.007419687\n",
      "Epoch: 12050 Lost= 0.010119942\n",
      "Epoch: 12100 Lost= 0.009455526\n",
      "Epoch: 12150 Lost= 0.007282944\n",
      "Epoch: 12200 Lost= 0.009563494\n",
      "Epoch: 12250 Lost= 0.010763662\n",
      "Epoch: 12300 Lost= 0.008284751\n",
      "Epoch: 12350 Lost= 0.013749707\n",
      "Epoch: 12400 Lost= 0.007861097\n",
      "Epoch: 12450 Lost= 0.008611571\n",
      "Epoch: 12500 Lost= 0.009021081\n",
      "Epoch: 12550 Lost= 0.010803984\n",
      "Epoch: 12600 Lost= 0.011546114\n",
      "Epoch: 12650 Lost= 0.008134741\n",
      "Epoch: 12700 Lost= 0.006357223\n",
      "Epoch: 12750 Lost= 0.004065423\n",
      "Epoch: 12800 Lost= 0.006415005\n",
      "Epoch: 12850 Lost= 0.009409066\n",
      "Epoch: 12900 Lost= 0.008390423\n",
      "Epoch: 12950 Lost= 0.004733874\n",
      "Epoch: 13000 Lost= 0.011217315\n",
      "Epoch: 13050 Lost= 0.006858189\n",
      "Epoch: 13100 Lost= 0.008750972\n",
      "Epoch: 13150 Lost= 0.009099538\n",
      "Epoch: 13200 Lost= 0.008622480\n",
      "Epoch: 13250 Lost= 0.005278785\n",
      "Epoch: 13300 Lost= 0.008175635\n",
      "Epoch: 13350 Lost= 0.008764725\n",
      "Epoch: 13400 Lost= 0.007879042\n",
      "Epoch: 13450 Lost= 0.007694568\n",
      "Epoch: 13500 Lost= 0.005896632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13550 Lost= 0.005632869\n",
      "Epoch: 13600 Lost= 0.006815027\n",
      "Epoch: 13650 Lost= 0.008118751\n",
      "Epoch: 13700 Lost= 0.011440285\n",
      "Epoch: 13750 Lost= 0.006162892\n",
      "Epoch: 13800 Lost= 0.009953838\n",
      "Epoch: 13850 Lost= 0.011738611\n",
      "Epoch: 13900 Lost= 0.008021288\n",
      "Epoch: 13950 Lost= 0.007666994\n",
      "Epoch: 14000 Lost= 0.007709508\n",
      "Epoch: 14050 Lost= 0.006825515\n",
      "Epoch: 14100 Lost= 0.008911789\n",
      "Epoch: 14150 Lost= 0.009491833\n",
      "Epoch: 14200 Lost= 0.008693245\n",
      "Epoch: 14250 Lost= 0.008717421\n",
      "Epoch: 14300 Lost= 0.007930111\n",
      "Epoch: 14350 Lost= 0.009081643\n",
      "Epoch: 14400 Lost= 0.006065155\n",
      "Epoch: 14450 Lost= 0.008252647\n",
      "Epoch: 14500 Lost= 0.008090321\n",
      "Epoch: 14550 Lost= 0.011633094\n",
      "Epoch: 14600 Lost= 0.009234563\n",
      "Epoch: 14650 Lost= 0.011019535\n",
      "Epoch: 14700 Lost= 0.008076333\n",
      "Epoch: 14750 Lost= 0.007173483\n",
      "Epoch: 14800 Lost= 0.008158238\n",
      "Epoch: 14850 Lost= 0.006314776\n",
      "Epoch: 14900 Lost= 0.009901852\n",
      "Epoch: 14950 Lost= 0.006671177\n",
      "Epoch: 15000 Lost= 0.011196550\n",
      "Epoch: 15050 Lost= 0.007595782\n",
      "Epoch: 15100 Lost= 0.008669260\n",
      "Epoch: 15150 Lost= 0.007030178\n",
      "Epoch: 15200 Lost= 0.007888583\n",
      "Epoch: 15250 Lost= 0.008224642\n",
      "Epoch: 15300 Lost= 0.006398422\n",
      "Epoch: 15350 Lost= 0.010387309\n",
      "Epoch: 15400 Lost= 0.008519441\n",
      "Epoch: 15450 Lost= 0.007255910\n",
      "Epoch: 15500 Lost= 0.008619310\n",
      "Epoch: 15550 Lost= 0.009751271\n",
      "Epoch: 15600 Lost= 0.009664794\n",
      "Epoch: 15650 Lost= 0.004681990\n",
      "Epoch: 15700 Lost= 0.007288109\n",
      "Epoch: 15750 Lost= 0.008224731\n",
      "Epoch: 15800 Lost= 0.006105193\n",
      "Epoch: 15850 Lost= 0.006619933\n",
      "Epoch: 15900 Lost= 0.006528505\n",
      "Epoch: 15950 Lost= 0.010836734\n",
      "Epoch: 16000 Lost= 0.004245331\n",
      "Epoch: 16050 Lost= 0.010690385\n",
      "Epoch: 16100 Lost= 0.008691190\n",
      "Epoch: 16150 Lost= 0.008801777\n",
      "Epoch: 16200 Lost= 0.008582821\n",
      "Epoch: 16250 Lost= 0.006444814\n",
      "Epoch: 16300 Lost= 0.008787225\n",
      "Epoch: 16350 Lost= 0.010113161\n",
      "Epoch: 16400 Lost= 0.007988972\n",
      "Epoch: 16450 Lost= 0.007585292\n",
      "Epoch: 16500 Lost= 0.005196767\n",
      "Epoch: 16550 Lost= 0.005291726\n",
      "Epoch: 16600 Lost= 0.005855420\n",
      "Epoch: 16650 Lost= 0.008673565\n",
      "Epoch: 16700 Lost= 0.010480969\n",
      "Epoch: 16750 Lost= 0.009658052\n",
      "Epoch: 16800 Lost= 0.005910727\n",
      "Epoch: 16850 Lost= 0.007159193\n",
      "Epoch: 16900 Lost= 0.006584763\n",
      "Epoch: 16950 Lost= 0.007759813\n",
      "Epoch: 17000 Lost= 0.007161091\n",
      "Epoch: 17050 Lost= 0.006839586\n",
      "Epoch: 17100 Lost= 0.008528038\n",
      "Epoch: 17150 Lost= 0.008525142\n",
      "Epoch: 17200 Lost= 0.006918689\n",
      "Epoch: 17250 Lost= 0.008389527\n",
      "Epoch: 17300 Lost= 0.009054191\n",
      "Epoch: 17350 Lost= 0.006447807\n",
      "Epoch: 17400 Lost= 0.008356281\n",
      "Epoch: 17450 Lost= 0.006382369\n",
      "Epoch: 17500 Lost= 0.007411908\n",
      "Epoch: 17550 Lost= 0.009758162\n",
      "Epoch: 17600 Lost= 0.009648698\n",
      "Epoch: 17650 Lost= 0.008569672\n",
      "Epoch: 17700 Lost= 0.006724897\n",
      "Epoch: 17750 Lost= 0.006682491\n",
      "Epoch: 17800 Lost= 0.008414228\n",
      "Epoch: 17850 Lost= 0.006627129\n",
      "Epoch: 17900 Lost= 0.006448397\n",
      "Epoch: 17950 Lost= 0.007052175\n",
      "Epoch: 18000 Lost= 0.010277992\n",
      "Epoch: 18050 Lost= 0.013823734\n",
      "Epoch: 18100 Lost= 0.007739730\n",
      "Epoch: 18150 Lost= 0.008258928\n",
      "Epoch: 18200 Lost= 0.007263196\n",
      "Epoch: 18250 Lost= 0.010378522\n",
      "Epoch: 18300 Lost= 0.005518522\n",
      "Epoch: 18350 Lost= 0.007401308\n",
      "Epoch: 18400 Lost= 0.010191462\n",
      "Epoch: 18450 Lost= 0.008194676\n",
      "Epoch: 18500 Lost= 0.010672641\n",
      "Epoch: 18550 Lost= 0.007453391\n",
      "Epoch: 18600 Lost= 0.006926469\n",
      "Epoch: 18650 Lost= 0.009036080\n",
      "Epoch: 18700 Lost= 0.008690300\n",
      "Epoch: 18750 Lost= 0.008990512\n",
      "Epoch: 18800 Lost= 0.004642749\n",
      "Epoch: 18850 Lost= 0.007094288\n",
      "Epoch: 18900 Lost= 0.007976912\n",
      "Epoch: 18950 Lost= 0.009646975\n",
      "Epoch: 19000 Lost= 0.008580666\n",
      "Epoch: 19050 Lost= 0.008763414\n",
      "Epoch: 19100 Lost= 0.006653792\n",
      "Epoch: 19150 Lost= 0.010694223\n",
      "Epoch: 19200 Lost= 0.008488149\n",
      "Epoch: 19250 Lost= 0.007583165\n",
      "Epoch: 19300 Lost= 0.005703240\n",
      "Epoch: 19350 Lost= 0.009050949\n",
      "Epoch: 19400 Lost= 0.006040953\n",
      "Epoch: 19450 Lost= 0.006026058\n",
      "Epoch: 19500 Lost= 0.008721674\n",
      "Epoch: 19550 Lost= 0.004974591\n",
      "Epoch: 19600 Lost= 0.008289952\n",
      "Epoch: 19650 Lost= 0.007539289\n",
      "Epoch: 19700 Lost= 0.006596063\n",
      "Epoch: 19750 Lost= 0.008026061\n",
      "Epoch: 19800 Lost= 0.009661452\n",
      "Epoch: 19850 Lost= 0.008556547\n",
      "Epoch: 19900 Lost= 0.007195662\n",
      "Epoch: 19950 Lost= 0.011652670\n",
      "Epoch: 20000 Lost= 0.006543138\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "First Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Running first session\n",
    "print(\"Starting 1st session...\")\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Writer to record image, scalar, histogram and graph for display in tensorboard\n",
    "    writer = tf.summary.FileWriter(\"/tmp/tensorflow_logs\", sess.graph)  # create writer\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        rand_index = np.random.choice(len(X_train_std), size=batch_size)\n",
    "        X_rand = X_train_std[rand_index]\n",
    "        y_rand = np.transpose([y_train[rand_index]])\n",
    "        sess.run(optimizer, feed_dict={X: X_rand, y: y_rand})\n",
    "\n",
    "        train_temp_loss = sess.run(loss, feed_dict={X: X_rand, y: y_rand})\n",
    "        train_loss.append(np.sqrt(train_temp_loss))\n",
    "    \n",
    "        test_temp_loss = sess.run(loss, feed_dict={X: X_test_std, y: np.transpose([y_test])})\n",
    "        test_loss.append(np.sqrt(test_temp_loss))\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"Lost=\", \\\n",
    "                \"{:.9f}\".format(train_temp_loss))\n",
    "\n",
    "    # Close writer\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "        \n",
    "    # Save model weights to disk\n",
    "    save_path = saver.save(sess, model_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    print(\"First Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Plot loss (MSE) over time\n",
    "plt.plot(train_loss, 'k-', label='Train Loss')\n",
    "plt.plot(test_loss, 'r--', label='Test Loss')\n",
    "plt.title('Loss (MSE) per Generation')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard Graph\n",
    "\n",
    "\n",
    "What follows is the graph we have executed and all data about it. Note the \"save\" label and the several layers.\n",
    "\n",
    "\n",
    "![graph_4](../images/graph_4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a Tensorflow model\n",
    "\n",
    "So, now we have our model saved.\n",
    "\n",
    "Tensorflow model has four main files:\n",
    "* a) Meta graph:\n",
    "This is a protocol buffer which saves the complete Tensorflow graph; i.e. all variables, operations, collections etc. This file has .meta extension.\n",
    "\n",
    "\n",
    "* b) y c) Checkpoint files:\n",
    "It is a binary file which contains all the values of the weights, biases, gradients and all the other variables saved. Tensorflow has changed from version 0.11. Instead of a single .ckpt file, we have now two files: .index and .data file that contains our training variables. \n",
    "\n",
    "\n",
    "* d) Along with this, Tensorflow also has a file named checkpoint which simply keeps a record of latest checkpoint files saved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the model\n",
    "\n",
    "\n",
    "We can retrain the model as many times as we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 2nd session...\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model restored from file: /tmp/model.ckpt\n",
      "Epoch: 0050 Lost= 0.005041842\n",
      "Epoch: 0100 Lost= 0.009546805\n",
      "Epoch: 0150 Lost= 0.005775886\n",
      "Epoch: 0200 Lost= 0.007824454\n",
      "Epoch: 0250 Lost= 0.008580340\n",
      "Epoch: 0300 Lost= 0.007931459\n",
      "Epoch: 0350 Lost= 0.008970352\n",
      "Epoch: 0400 Lost= 0.007261851\n",
      "Epoch: 0450 Lost= 0.006378519\n",
      "Epoch: 0500 Lost= 0.007015410\n",
      "Epoch: 0550 Lost= 0.008605658\n",
      "Epoch: 0600 Lost= 0.005657943\n",
      "Epoch: 0650 Lost= 0.008467887\n",
      "Epoch: 0700 Lost= 0.006358911\n",
      "Epoch: 0750 Lost= 0.007192065\n",
      "Epoch: 0800 Lost= 0.010194425\n",
      "Epoch: 0850 Lost= 0.010251625\n",
      "Epoch: 0900 Lost= 0.006081640\n",
      "Epoch: 0950 Lost= 0.008559209\n",
      "Epoch: 1000 Lost= 0.008426270\n",
      "Epoch: 1050 Lost= 0.006850173\n",
      "Epoch: 1100 Lost= 0.006602024\n",
      "Epoch: 1150 Lost= 0.005525755\n",
      "Epoch: 1200 Lost= 0.004971796\n",
      "Epoch: 1250 Lost= 0.005323414\n",
      "Epoch: 1300 Lost= 0.008665145\n",
      "Epoch: 1350 Lost= 0.005478811\n",
      "Epoch: 1400 Lost= 0.008417126\n",
      "Epoch: 1450 Lost= 0.009698820\n",
      "Epoch: 1500 Lost= 0.007273917\n",
      "Epoch: 1550 Lost= 0.009147751\n",
      "Epoch: 1600 Lost= 0.008690137\n",
      "Epoch: 1650 Lost= 0.004498571\n",
      "Epoch: 1700 Lost= 0.006312715\n",
      "Epoch: 1750 Lost= 0.006543590\n",
      "Epoch: 1800 Lost= 0.008354567\n",
      "Epoch: 1850 Lost= 0.008887324\n",
      "Epoch: 1900 Lost= 0.008673258\n",
      "Epoch: 1950 Lost= 0.005497400\n",
      "Epoch: 2000 Lost= 0.007656193\n",
      "Epoch: 2050 Lost= 0.008252309\n",
      "Epoch: 2100 Lost= 0.010009023\n",
      "Epoch: 2150 Lost= 0.006485737\n",
      "Epoch: 2200 Lost= 0.007600236\n",
      "Epoch: 2250 Lost= 0.007890484\n",
      "Epoch: 2300 Lost= 0.010686139\n",
      "Epoch: 2350 Lost= 0.009467045\n",
      "Epoch: 2400 Lost= 0.006263439\n",
      "Epoch: 2450 Lost= 0.009795585\n",
      "Epoch: 2500 Lost= 0.006680989\n",
      "Epoch: 2550 Lost= 0.006302079\n",
      "Epoch: 2600 Lost= 0.009419059\n",
      "Epoch: 2650 Lost= 0.010173585\n",
      "Epoch: 2700 Lost= 0.006527126\n",
      "Epoch: 2750 Lost= 0.007234990\n",
      "Epoch: 2800 Lost= 0.008705299\n",
      "Epoch: 2850 Lost= 0.011993771\n",
      "Epoch: 2900 Lost= 0.007840832\n",
      "Epoch: 2950 Lost= 0.007629772\n",
      "Epoch: 3000 Lost= 0.007258614\n",
      "Epoch: 3050 Lost= 0.005128348\n",
      "Epoch: 3100 Lost= 0.006027864\n",
      "Epoch: 3150 Lost= 0.009242267\n",
      "Epoch: 3200 Lost= 0.009412067\n",
      "Epoch: 3250 Lost= 0.008244288\n",
      "Epoch: 3300 Lost= 0.008265602\n",
      "Epoch: 3350 Lost= 0.006170918\n",
      "Epoch: 3400 Lost= 0.008332644\n",
      "Epoch: 3450 Lost= 0.008046381\n",
      "Epoch: 3500 Lost= 0.011779525\n",
      "Epoch: 3550 Lost= 0.007494439\n",
      "Epoch: 3600 Lost= 0.004192307\n",
      "Epoch: 3650 Lost= 0.008965132\n",
      "Epoch: 3700 Lost= 0.005680904\n",
      "Epoch: 3750 Lost= 0.007717081\n",
      "Epoch: 3800 Lost= 0.007106786\n",
      "Epoch: 3850 Lost= 0.006457070\n",
      "Epoch: 3900 Lost= 0.008232861\n",
      "Epoch: 3950 Lost= 0.009357920\n",
      "Epoch: 4000 Lost= 0.004234340\n",
      "Epoch: 4050 Lost= 0.009148141\n",
      "Epoch: 4100 Lost= 0.008343216\n",
      "Epoch: 4150 Lost= 0.011350679\n",
      "Epoch: 4200 Lost= 0.008573718\n",
      "Epoch: 4250 Lost= 0.008958365\n",
      "Epoch: 4300 Lost= 0.010270356\n",
      "Epoch: 4350 Lost= 0.006870217\n",
      "Epoch: 4400 Lost= 0.010366952\n",
      "Epoch: 4450 Lost= 0.008084949\n",
      "Epoch: 4500 Lost= 0.005464375\n",
      "Epoch: 4550 Lost= 0.008884406\n",
      "Epoch: 4600 Lost= 0.008820341\n",
      "Epoch: 4650 Lost= 0.007857248\n",
      "Epoch: 4700 Lost= 0.008900041\n",
      "Epoch: 4750 Lost= 0.006080193\n",
      "Epoch: 4800 Lost= 0.007853830\n",
      "Epoch: 4850 Lost= 0.005644011\n",
      "Epoch: 4900 Lost= 0.006562014\n",
      "Epoch: 4950 Lost= 0.006303832\n",
      "Epoch: 5000 Lost= 0.007936259\n",
      "Epoch: 5050 Lost= 0.004696697\n",
      "Epoch: 5100 Lost= 0.007718727\n",
      "Epoch: 5150 Lost= 0.007507156\n",
      "Epoch: 5200 Lost= 0.006849275\n",
      "Epoch: 5250 Lost= 0.008418744\n",
      "Epoch: 5300 Lost= 0.006487692\n",
      "Epoch: 5350 Lost= 0.007816028\n",
      "Epoch: 5400 Lost= 0.006939635\n",
      "Epoch: 5450 Lost= 0.007709744\n",
      "Epoch: 5500 Lost= 0.005502835\n",
      "Epoch: 5550 Lost= 0.009906376\n",
      "Epoch: 5600 Lost= 0.010553815\n",
      "Epoch: 5650 Lost= 0.008167796\n",
      "Epoch: 5700 Lost= 0.007932858\n",
      "Epoch: 5750 Lost= 0.005611158\n",
      "Epoch: 5800 Lost= 0.011713042\n",
      "Epoch: 5850 Lost= 0.011109651\n",
      "Epoch: 5900 Lost= 0.010467094\n",
      "Epoch: 5950 Lost= 0.005631764\n",
      "Epoch: 6000 Lost= 0.008384974\n",
      "Epoch: 6050 Lost= 0.006564800\n",
      "Epoch: 6100 Lost= 0.006650037\n",
      "Epoch: 6150 Lost= 0.008816515\n",
      "Epoch: 6200 Lost= 0.006979877\n",
      "Epoch: 6250 Lost= 0.004099650\n",
      "Epoch: 6300 Lost= 0.008083394\n",
      "Epoch: 6350 Lost= 0.005835733\n",
      "Epoch: 6400 Lost= 0.006880307\n",
      "Epoch: 6450 Lost= 0.006608414\n",
      "Epoch: 6500 Lost= 0.007656022\n",
      "Epoch: 6550 Lost= 0.006414623\n",
      "Epoch: 6600 Lost= 0.011735180\n",
      "Epoch: 6650 Lost= 0.010311360\n",
      "Epoch: 6700 Lost= 0.009221880\n",
      "Epoch: 6750 Lost= 0.006877968\n",
      "Epoch: 6800 Lost= 0.008097995\n",
      "Epoch: 6850 Lost= 0.005890036\n",
      "Epoch: 6900 Lost= 0.004208353\n",
      "Epoch: 6950 Lost= 0.009363938\n",
      "Epoch: 7000 Lost= 0.003956562\n",
      "Epoch: 7050 Lost= 0.006360681\n",
      "Epoch: 7100 Lost= 0.009188264\n",
      "Epoch: 7150 Lost= 0.004961106\n",
      "Epoch: 7200 Lost= 0.005965902\n",
      "Epoch: 7250 Lost= 0.009310879\n",
      "Epoch: 7300 Lost= 0.009005233\n",
      "Epoch: 7350 Lost= 0.007745225\n",
      "Epoch: 7400 Lost= 0.004404296\n",
      "Epoch: 7450 Lost= 0.007501466\n",
      "Epoch: 7500 Lost= 0.008695258\n",
      "Epoch: 7550 Lost= 0.012870871\n",
      "Epoch: 7600 Lost= 0.004114454\n",
      "Epoch: 7650 Lost= 0.005807302\n",
      "Epoch: 7700 Lost= 0.006074358\n",
      "Epoch: 7750 Lost= 0.009134257\n",
      "Epoch: 7800 Lost= 0.007534883\n",
      "Epoch: 7850 Lost= 0.005949765\n",
      "Epoch: 7900 Lost= 0.006410717\n",
      "Epoch: 7950 Lost= 0.008438864\n",
      "Epoch: 8000 Lost= 0.010682238\n",
      "Epoch: 8050 Lost= 0.006581264\n",
      "Epoch: 8100 Lost= 0.008373315\n",
      "Epoch: 8150 Lost= 0.009552897\n",
      "Epoch: 8200 Lost= 0.005419523\n",
      "Epoch: 8250 Lost= 0.007900761\n",
      "Epoch: 8300 Lost= 0.008119849\n",
      "Epoch: 8350 Lost= 0.010247719\n",
      "Epoch: 8400 Lost= 0.008069276\n",
      "Epoch: 8450 Lost= 0.009736118\n",
      "Epoch: 8500 Lost= 0.005102023\n",
      "Epoch: 8550 Lost= 0.008546787\n",
      "Epoch: 8600 Lost= 0.006270694\n",
      "Epoch: 8650 Lost= 0.004386835\n",
      "Epoch: 8700 Lost= 0.005675018\n",
      "Epoch: 8750 Lost= 0.004773800\n",
      "Epoch: 8800 Lost= 0.005307519\n",
      "Epoch: 8850 Lost= 0.007210653\n",
      "Epoch: 8900 Lost= 0.008422696\n",
      "Epoch: 8950 Lost= 0.010247355\n",
      "Epoch: 9000 Lost= 0.006301480\n",
      "Epoch: 9050 Lost= 0.010536982\n",
      "Epoch: 9100 Lost= 0.007577286\n",
      "Epoch: 9150 Lost= 0.008384489\n",
      "Epoch: 9200 Lost= 0.005966158\n",
      "Epoch: 9250 Lost= 0.007340459\n",
      "Epoch: 9300 Lost= 0.008134501\n",
      "Epoch: 9350 Lost= 0.007511449\n",
      "Epoch: 9400 Lost= 0.010021457\n",
      "Epoch: 9450 Lost= 0.006297401\n",
      "Epoch: 9500 Lost= 0.006344876\n",
      "Epoch: 9550 Lost= 0.007496977\n",
      "Epoch: 9600 Lost= 0.005550491\n",
      "Epoch: 9650 Lost= 0.006365268\n",
      "Epoch: 9700 Lost= 0.006376990\n",
      "Epoch: 9750 Lost= 0.007669856\n",
      "Epoch: 9800 Lost= 0.006655696\n",
      "Epoch: 9850 Lost= 0.006881214\n",
      "Epoch: 9900 Lost= 0.009258513\n",
      "Epoch: 9950 Lost= 0.007117258\n",
      "Epoch: 10000 Lost= 0.006624982\n",
      "Epoch: 10050 Lost= 0.005769862\n",
      "Epoch: 10100 Lost= 0.007976382\n",
      "Epoch: 10150 Lost= 0.007775109\n",
      "Epoch: 10200 Lost= 0.004919123\n",
      "Epoch: 10250 Lost= 0.006236374\n",
      "Epoch: 10300 Lost= 0.004792977\n",
      "Epoch: 10350 Lost= 0.005501995\n",
      "Epoch: 10400 Lost= 0.010107455\n",
      "Epoch: 10450 Lost= 0.007887789\n",
      "Epoch: 10500 Lost= 0.009176884\n",
      "Epoch: 10550 Lost= 0.005542694\n",
      "Epoch: 10600 Lost= 0.007474746\n",
      "Epoch: 10650 Lost= 0.006854836\n",
      "Epoch: 10700 Lost= 0.006394846\n",
      "Epoch: 10750 Lost= 0.008457692\n",
      "Epoch: 10800 Lost= 0.007503803\n",
      "Epoch: 10850 Lost= 0.008877346\n",
      "Epoch: 10900 Lost= 0.004673202\n",
      "Epoch: 10950 Lost= 0.009199435\n",
      "Epoch: 11000 Lost= 0.009489781\n",
      "Epoch: 11050 Lost= 0.005105094\n",
      "Epoch: 11100 Lost= 0.007840242\n",
      "Epoch: 11150 Lost= 0.008145290\n",
      "Epoch: 11200 Lost= 0.006067829\n",
      "Epoch: 11250 Lost= 0.006646139\n",
      "Epoch: 11300 Lost= 0.009113736\n",
      "Epoch: 11350 Lost= 0.006060565\n",
      "Epoch: 11400 Lost= 0.006180578\n",
      "Epoch: 11450 Lost= 0.007558454\n",
      "Epoch: 11500 Lost= 0.005352644\n",
      "Epoch: 11550 Lost= 0.009554598\n",
      "Epoch: 11600 Lost= 0.008687596\n",
      "Epoch: 11650 Lost= 0.009030865\n",
      "Epoch: 11700 Lost= 0.009860888\n",
      "Epoch: 11750 Lost= 0.008384689\n",
      "Epoch: 11800 Lost= 0.005513561\n",
      "Epoch: 11850 Lost= 0.006891503\n",
      "Epoch: 11900 Lost= 0.010220263\n",
      "Epoch: 11950 Lost= 0.008402021\n",
      "Epoch: 12000 Lost= 0.004952035\n",
      "Epoch: 12050 Lost= 0.005986688\n",
      "Epoch: 12100 Lost= 0.004907650\n",
      "Epoch: 12150 Lost= 0.006461366\n",
      "Epoch: 12200 Lost= 0.006678347\n",
      "Epoch: 12250 Lost= 0.007392884\n",
      "Epoch: 12300 Lost= 0.007413558\n",
      "Epoch: 12350 Lost= 0.008151393\n",
      "Epoch: 12400 Lost= 0.009229386\n",
      "Epoch: 12450 Lost= 0.006453891\n",
      "Epoch: 12500 Lost= 0.007121006\n",
      "Epoch: 12550 Lost= 0.008219780\n",
      "Epoch: 12600 Lost= 0.006328676\n",
      "Epoch: 12650 Lost= 0.007513509\n",
      "Epoch: 12700 Lost= 0.004610452\n",
      "Epoch: 12750 Lost= 0.005722724\n",
      "Epoch: 12800 Lost= 0.008055407\n",
      "Epoch: 12850 Lost= 0.005942039\n",
      "Epoch: 12900 Lost= 0.008335205\n",
      "Epoch: 12950 Lost= 0.003686266\n",
      "Epoch: 13000 Lost= 0.008346436\n",
      "Epoch: 13050 Lost= 0.007907318\n",
      "Epoch: 13100 Lost= 0.007345054\n",
      "Epoch: 13150 Lost= 0.007315617\n",
      "Epoch: 13200 Lost= 0.008408464\n",
      "Epoch: 13250 Lost= 0.011262035\n",
      "Epoch: 13300 Lost= 0.009228424\n",
      "Epoch: 13350 Lost= 0.006318146\n",
      "Epoch: 13400 Lost= 0.005968017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13450 Lost= 0.005479197\n",
      "Epoch: 13500 Lost= 0.011259765\n",
      "Epoch: 13550 Lost= 0.007103911\n",
      "Epoch: 13600 Lost= 0.007041028\n",
      "Epoch: 13650 Lost= 0.007675832\n",
      "Epoch: 13700 Lost= 0.006710136\n",
      "Epoch: 13750 Lost= 0.006218206\n",
      "Epoch: 13800 Lost= 0.007263848\n",
      "Epoch: 13850 Lost= 0.007433532\n",
      "Epoch: 13900 Lost= 0.006390058\n",
      "Epoch: 13950 Lost= 0.007822157\n",
      "Epoch: 14000 Lost= 0.007078342\n",
      "Epoch: 14050 Lost= 0.003960340\n",
      "Epoch: 14100 Lost= 0.007227399\n",
      "Epoch: 14150 Lost= 0.007903156\n",
      "Epoch: 14200 Lost= 0.007028466\n",
      "Epoch: 14250 Lost= 0.008471079\n",
      "Epoch: 14300 Lost= 0.008928332\n",
      "Epoch: 14350 Lost= 0.010615224\n",
      "Epoch: 14400 Lost= 0.008843523\n",
      "Epoch: 14450 Lost= 0.006483784\n",
      "Epoch: 14500 Lost= 0.009713305\n",
      "Epoch: 14550 Lost= 0.007571382\n",
      "Epoch: 14600 Lost= 0.005843386\n",
      "Epoch: 14650 Lost= 0.005052203\n",
      "Epoch: 14700 Lost= 0.006392506\n",
      "Epoch: 14750 Lost= 0.005964449\n",
      "Epoch: 14800 Lost= 0.003595609\n",
      "Epoch: 14850 Lost= 0.006841784\n",
      "Epoch: 14900 Lost= 0.008828690\n",
      "Epoch: 14950 Lost= 0.006058370\n",
      "Epoch: 15000 Lost= 0.004471452\n",
      "Epoch: 15050 Lost= 0.007304328\n",
      "Epoch: 15100 Lost= 0.006468690\n",
      "Epoch: 15150 Lost= 0.007744065\n",
      "Epoch: 15200 Lost= 0.006920145\n",
      "Epoch: 15250 Lost= 0.005351207\n",
      "Epoch: 15300 Lost= 0.006249478\n",
      "Epoch: 15350 Lost= 0.007862034\n",
      "Epoch: 15400 Lost= 0.006703055\n",
      "Epoch: 15450 Lost= 0.007223462\n",
      "Epoch: 15500 Lost= 0.009107986\n",
      "Epoch: 15550 Lost= 0.005296415\n",
      "Epoch: 15600 Lost= 0.007799071\n",
      "Epoch: 15650 Lost= 0.007058540\n",
      "Epoch: 15700 Lost= 0.007415156\n",
      "Epoch: 15750 Lost= 0.004754341\n",
      "Epoch: 15800 Lost= 0.009977859\n",
      "Epoch: 15850 Lost= 0.005707975\n",
      "Epoch: 15900 Lost= 0.006344942\n",
      "Epoch: 15950 Lost= 0.007073890\n",
      "Epoch: 16000 Lost= 0.004957111\n",
      "Epoch: 16050 Lost= 0.005932834\n",
      "Epoch: 16100 Lost= 0.007885623\n",
      "Epoch: 16150 Lost= 0.007668026\n",
      "Epoch: 16200 Lost= 0.005774815\n",
      "Epoch: 16250 Lost= 0.006901270\n",
      "Epoch: 16300 Lost= 0.009778908\n",
      "Epoch: 16350 Lost= 0.006913783\n",
      "Epoch: 16400 Lost= 0.009037969\n",
      "Epoch: 16450 Lost= 0.007830893\n",
      "Epoch: 16500 Lost= 0.010275578\n",
      "Epoch: 16550 Lost= 0.006407964\n",
      "Epoch: 16600 Lost= 0.008339969\n",
      "Epoch: 16650 Lost= 0.007295711\n",
      "Epoch: 16700 Lost= 0.008105588\n",
      "Epoch: 16750 Lost= 0.007185924\n",
      "Epoch: 16800 Lost= 0.005154172\n",
      "Epoch: 16850 Lost= 0.007645465\n",
      "Epoch: 16900 Lost= 0.003857441\n",
      "Epoch: 16950 Lost= 0.006040929\n",
      "Epoch: 17000 Lost= 0.005449048\n",
      "Epoch: 17050 Lost= 0.005528703\n",
      "Epoch: 17100 Lost= 0.005872307\n",
      "Epoch: 17150 Lost= 0.005877674\n",
      "Epoch: 17200 Lost= 0.009322589\n",
      "Epoch: 17250 Lost= 0.008123465\n",
      "Epoch: 17300 Lost= 0.009965479\n",
      "Epoch: 17350 Lost= 0.005616601\n",
      "Epoch: 17400 Lost= 0.009574986\n",
      "Epoch: 17450 Lost= 0.009350177\n",
      "Epoch: 17500 Lost= 0.007447958\n",
      "Epoch: 17550 Lost= 0.007152820\n",
      "Epoch: 17600 Lost= 0.007281182\n",
      "Epoch: 17650 Lost= 0.006191325\n",
      "Epoch: 17700 Lost= 0.007379348\n",
      "Epoch: 17750 Lost= 0.006422833\n",
      "Epoch: 17800 Lost= 0.007270728\n",
      "Epoch: 17850 Lost= 0.007101817\n",
      "Epoch: 17900 Lost= 0.006515443\n",
      "Epoch: 17950 Lost= 0.005924327\n",
      "Epoch: 18000 Lost= 0.004593258\n",
      "Epoch: 18050 Lost= 0.005201382\n",
      "Epoch: 18100 Lost= 0.007536802\n",
      "Epoch: 18150 Lost= 0.009913336\n",
      "Epoch: 18200 Lost= 0.008249153\n",
      "Epoch: 18250 Lost= 0.008396115\n",
      "Epoch: 18300 Lost= 0.006929477\n",
      "Epoch: 18350 Lost= 0.008714084\n",
      "Epoch: 18400 Lost= 0.008800382\n",
      "Epoch: 18450 Lost= 0.005674787\n",
      "Epoch: 18500 Lost= 0.007611227\n",
      "Epoch: 18550 Lost= 0.006444989\n",
      "Epoch: 18600 Lost= 0.006130911\n",
      "Epoch: 18650 Lost= 0.006573578\n",
      "Epoch: 18700 Lost= 0.007453994\n",
      "Epoch: 18750 Lost= 0.005215432\n",
      "Epoch: 18800 Lost= 0.006795323\n",
      "Epoch: 18850 Lost= 0.008568337\n",
      "Epoch: 18900 Lost= 0.005897449\n",
      "Epoch: 18950 Lost= 0.007182786\n",
      "Epoch: 19000 Lost= 0.006120993\n",
      "Epoch: 19050 Lost= 0.008071944\n",
      "Epoch: 19100 Lost= 0.006324450\n",
      "Epoch: 19150 Lost= 0.006784413\n",
      "Epoch: 19200 Lost= 0.008289578\n",
      "Epoch: 19250 Lost= 0.006005373\n",
      "Epoch: 19300 Lost= 0.006271440\n",
      "Epoch: 19350 Lost= 0.008909656\n",
      "Epoch: 19400 Lost= 0.006640341\n",
      "Epoch: 19450 Lost= 0.005713032\n",
      "Epoch: 19500 Lost= 0.007720225\n",
      "Epoch: 19550 Lost= 0.004735262\n",
      "Epoch: 19600 Lost= 0.008761092\n",
      "Epoch: 19650 Lost= 0.006547910\n",
      "Epoch: 19700 Lost= 0.007753095\n",
      "Epoch: 19750 Lost= 0.006880271\n",
      "Epoch: 19800 Lost= 0.006767692\n",
      "Epoch: 19850 Lost= 0.007084911\n",
      "Epoch: 19900 Lost= 0.006260275\n",
      "Epoch: 19950 Lost= 0.007726617\n",
      "Epoch: 20000 Lost= 0.004768252\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Second Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Running a new session\n",
    "print(\"Starting 2nd session...\")\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "\n",
    "    # Restore model weights from previously saved model\n",
    "    saver.restore(sess, model_path)\n",
    "    print(\"Model restored from file: %s\" % model_path)\n",
    "\n",
    "    # Resume training\n",
    "    for epoch in range(epochs):\n",
    "        rand_index = np.random.choice(len(X_train), size=batch_size)\n",
    "        X_rand = X_train_std[rand_index]\n",
    "        y_rand = np.transpose([y_train[rand_index]])\n",
    "        sess.run(optimizer, feed_dict={X: X_rand, y: y_rand})\n",
    "\n",
    "        train_temp_loss = sess.run(loss, feed_dict={X: X_rand, y: y_rand})\n",
    "        train_loss.append(np.sqrt(train_temp_loss))\n",
    "    \n",
    "        test_temp_loss = sess.run(loss, feed_dict={X: X_test_std, y: np.transpose([y_test])})\n",
    "        test_loss.append(np.sqrt(test_temp_loss))\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"Lost=\", \\\n",
    "                \"{:.9f}\".format(train_temp_loss))\n",
    "\n",
    "    # Close writer\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "    # Save model weights to disk\n",
    "    save_path = saver.save(sess, model_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    print(\"Second Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.2, 0.1,\n",
       "       0.1, 0.2, 0.4, 0.4, 0.3, 0.3, 0.3, 0.2, 0.4, 0.2, 0.5, 0.2, 0.2,\n",
       "       0.4, 0.2, 0.2, 0.2, 0.2, 0.4, 0.1, 0.2, 0.1, 0.2, 0.2, 0.1, 0.2,\n",
       "       0.2, 0.3, 0.3, 0.2, 0.6, 0.4, 0.3, 0.2, 0.2, 0.2, 0.2, 1.4, 1.5,\n",
       "       1.5, 1.3, 1.5, 1.3, 1.6, 1. , 1.3, 1.4, 1. , 1.5, 1. , 1.4, 1.3,\n",
       "       1.4, 1.5, 1. , 1.5, 1.1, 1.8, 1.3, 1.5, 1.2, 1.3, 1.4, 1.4, 1.7,\n",
       "       1.5, 1. , 1.1, 1. , 1.2, 1.6, 1.5, 1.6, 1.5, 1.3, 1.3, 1.3, 1.2,\n",
       "       1.4, 1.2, 1. , 1.3, 1.2, 1.3, 1.3, 1.1, 1.3, 2.5, 1.9, 2.1, 1.8,\n",
       "       2.2, 2.1, 1.7, 1.8, 1.8, 2.5, 2. , 1.9, 2.1, 2. , 2.4, 2.3, 1.8,\n",
       "       2.2, 2.3, 1.5, 2.3, 2. , 2. , 1.8, 2.1, 1.8, 1.8, 1.8, 2.1, 1.6,\n",
       "       1.9, 2. , 2.2, 1.5, 1.4, 2.3, 2.4, 1.8, 1.8, 2.1, 2.4, 2.3, 1.9,\n",
       "       2.3, 2.5, 2.3, 1.9, 2. , 2.3, 1.8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the model to make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction session...\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model restored from file: /tmp/model.ckpt\n",
      "[[2.3523307]\n",
      " [2.2160456]\n",
      " [5.1773496]]\n"
     ]
    }
   ],
   "source": [
    "# Running a new session for predictions\n",
    "print(\"Starting prediction session...\")\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "\n",
    "    # Restore model weights from previously saved model\n",
    "    saver.restore(sess, model_path)\n",
    "    print(\"Model restored from file: %s\" % model_path)\n",
    "\n",
    "    # We try to predict the petal width (cm) of three samples\n",
    "    feed_dict = {X: [[5.1, 3.5, 1.4],\n",
    "                     [4.8, 3.0, 1.4],\n",
    "                     [6.3, 3.4, 5.6]]\n",
    "                }\n",
    "    prediction = sess.run(y_hat, feed_dict)\n",
    "    print(prediction) # True value 0.2, 0.1, 2.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, better results, but still not very good results. We could try to improve them with a deeper network (more layers) or retouching the net parameters and number of neurons. That is another story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction session...\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model restored from file: /tmp/model.ckpt\n",
      "[[2.6163282]\n",
      " [2.42447  ]\n",
      " [6.719332 ]]\n"
     ]
    }
   ],
   "source": [
    "# Running a new session for predictions\n",
    "print(\"Starting prediction session...\")\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "\n",
    "    # Restore model weights from previously saved model\n",
    "    saver.restore(sess, model_path)\n",
    "    print(\"Model restored from file: %s\" % model_path)\n",
    "\n",
    "    # We try to predict the petal width (cm) of three samples\n",
    "    feed_dict_std = {X: [[6.86549436, 4.28228483, 0.89182234],\n",
    "       [6.38114257, 3.47503186, 0.89182234],\n",
    "       [8.8029015 , 4.12083424, 7.67274733]]}\n",
    "    prediction = sess.run(y_hat, feed_dict_std)\n",
    "    print(prediction) # True value 0.2, 0.1, 2.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_rev = sc.inverse_transform(prediction)\n",
    "y_hat_rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not good. We'll see"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deeptrading",
   "language": "python",
   "name": "deeptrading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
